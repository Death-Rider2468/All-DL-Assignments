{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "APiViPI_y4mT"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas numpy tensorflow keras\n",
        "#!pip install --upgrade pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load datasets\n",
        "#Updated to try except block to handle older version of pandas\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/train.csv', quoting=pd.QUOTE_NONNUMERIC, on_bad_lines='skip')\n",
        "    test_df = pd.read_csv('/content/test.csv', quoting=pd.QUOTE_NONNUMERIC, on_bad_lines='skip')\n",
        "except AttributeError:\n",
        "    train_df = pd.read_csv('/content/train.csv', on_bad_lines='skip')  # For pandas versions < 1.0\n",
        "    test_df = pd.read_csv('/content/test.csv', on_bad_lines='skip')  # For pandas versions < 1.0\n",
        "\n",
        "# Extract texts and titles\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_titles = train_df['title'].astype(str).tolist()\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "test_titles = test_df['title'].astype(str).tolist()\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "# Extract texts and titles\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_titles = train_df['title'].astype(str).tolist()\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "test_titles = test_df['title'].astype(str).tolist()\n",
        "\n",
        "# Add <start> and <end> tokens to titles for sequence-to-sequence learning\n",
        "#train_titles = ['<start> ' + title + ' <end>' for title in train_titles]\n",
        "#test_titles = ['<start> ' + title + ' <end>' for title in test_titles]\n",
        "\n",
        "# Tokenization parameters\n",
        "vocab_size = 20000\n",
        "max_text_len = 400\n",
        "max_title_len = 20  # include <start> and <end> tokens within this length\n",
        "\n",
        "\n",
        "# Tokenizer for texts\n",
        "text_tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "text_tokenizer.fit_on_texts(train_texts)\n",
        "train_text_seq = text_tokenizer.texts_to_sequences(train_texts)\n",
        "train_text_seq = pad_sequences(train_text_seq, maxlen=max_text_len, padding='post')\n",
        "test_text_seq = text_tokenizer.texts_to_sequences(test_texts)\n",
        "test_text_seq = pad_sequences(test_text_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# Tokenizer for titles\n",
        "title_tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>', filters='')\n",
        "title_tokenizer.fit_on_texts(train_titles)\n",
        "train_title_seq = title_tokenizer.texts_to_sequences(train_titles)\n",
        "train_title_seq = pad_sequences(train_title_seq, maxlen=max_title_len, padding='post')\n",
        "test_title_seq = title_tokenizer.texts_to_sequences(test_titles)\n",
        "test_title_seq = pad_sequences(test_title_seq, maxlen=max_title_len, padding='post')\n",
        "\n",
        "# Prepare decoder input and output sequences\n",
        "decoder_input_seq = train_title_seq[:, :-1]\n",
        "decoder_target_seq = train_title_seq[:, 1:]\n",
        "\n",
        "# Prepare decoder input for test set (used in evaluation and later generation)\n",
        "decoder_input_test_seq = test_title_seq[:, :-1]\n",
        "decoder_target_test_seq = test_title_seq[:, 1:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 256\n",
        "lstm_units = 512\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "enc_emb = Embedding(vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_title_len - 1,))\n",
        "dec_emb_layer = Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model_no_attention = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model_no_attention.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_no_attention.summary()\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "model_no_attention.fit([train_text_seq, decoder_input_seq], decoder_target_seq, batch_size=100, epochs=30, validation_split=0.2)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training time: {training_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate BLEU Score\n",
        "def evaluate_bleu(model, inputs, targets, tokenizer):\n",
        "    predictions = model.predict(inputs)\n",
        "    predicted_ids = np.argmax(predictions, axis=-1)\n",
        "    references = [[tokenizer.index_word.get(id, '') for id in target if id != 0] for target in targets]\n",
        "    hypotheses = [[tokenizer.index_word.get(id, '') for id in pred if id != 0] for pred in predicted_ids]\n",
        "    scores = [sentence_bleu([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Inference time measurement\n",
        "start_inf_time = time.time()\n",
        "_ = model_no_attention.predict([test_text_seq[:1], test_title_seq[:, :-1][:1]])\n",
        "inference_time = time.time() - start_inf_time\n",
        "print(f\"Inference time per sample: {inference_time:.4f} seconds\")\n",
        "\n",
        "# BLEU score\n",
        "bleu_score_no_attention = evaluate_bleu(model_no_attention, [test_text_seq, test_title_seq[:, :-1]], test_title_seq[:, 1:], title_tokenizer)\n",
        "print(f\"BLEU Score on Test Set: {bleu_score_no_attention:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sqWuvUT6zLka",
        "outputId": "b0572fc5-cde2-48a8-9184-db93e7c9f968"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_9       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_10      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_4         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m5,120,000\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m5,120,000\u001b[0m │ input_layer_10[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),     │  \u001b[38;5;34m1,574,912\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m512\u001b[0m), │  \u001b[38;5;34m1,574,912\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m20000\u001b[0m) │ \u001b[38;5;34m10,260,000\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_9       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_10      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_4         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │ input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,260,000</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,649,824\u001b[0m (90.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,649,824</span> (90.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,649,824\u001b[0m (90.22 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,649,824</span> (90.22 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 295ms/step - accuracy: 0.4863 - loss: 8.1839 - val_accuracy: 0.5426 - val_loss: 7.1416\n",
            "Epoch 2/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 278ms/step - accuracy: 0.5442 - loss: 6.8908 - val_accuracy: 0.5480 - val_loss: 6.9997\n",
            "Epoch 3/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 284ms/step - accuracy: 0.5509 - loss: 6.5935 - val_accuracy: 0.5529 - val_loss: 6.8857\n",
            "Epoch 4/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 289ms/step - accuracy: 0.5581 - loss: 6.3260 - val_accuracy: 0.5561 - val_loss: 6.8003\n",
            "Epoch 5/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 308ms/step - accuracy: 0.5661 - loss: 6.0329 - val_accuracy: 0.5601 - val_loss: 6.7335\n",
            "Epoch 6/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 315ms/step - accuracy: 0.5747 - loss: 5.7429 - val_accuracy: 0.5649 - val_loss: 6.7034\n",
            "Epoch 7/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 305ms/step - accuracy: 0.5846 - loss: 5.4550 - val_accuracy: 0.5672 - val_loss: 6.6867\n",
            "Epoch 8/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 316ms/step - accuracy: 0.5921 - loss: 5.1789 - val_accuracy: 0.5688 - val_loss: 6.7004\n",
            "Epoch 9/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 304ms/step - accuracy: 0.5977 - loss: 4.8973 - val_accuracy: 0.5716 - val_loss: 6.7152\n",
            "Epoch 10/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 302ms/step - accuracy: 0.6077 - loss: 4.6087 - val_accuracy: 0.5704 - val_loss: 6.7322\n",
            "Epoch 11/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 302ms/step - accuracy: 0.6176 - loss: 4.3458 - val_accuracy: 0.5713 - val_loss: 6.7707\n",
            "Epoch 12/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 318ms/step - accuracy: 0.6316 - loss: 4.0807 - val_accuracy: 0.5696 - val_loss: 6.8269\n",
            "Epoch 13/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - accuracy: 0.6468 - loss: 3.8275 - val_accuracy: 0.5706 - val_loss: 6.8841\n",
            "Epoch 14/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 303ms/step - accuracy: 0.6644 - loss: 3.5733 - val_accuracy: 0.5703 - val_loss: 6.9489\n",
            "Epoch 15/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 318ms/step - accuracy: 0.6835 - loss: 3.3369 - val_accuracy: 0.5718 - val_loss: 7.0317\n",
            "Epoch 16/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 318ms/step - accuracy: 0.7006 - loss: 3.1114 - val_accuracy: 0.5713 - val_loss: 7.1035\n",
            "Epoch 17/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - accuracy: 0.7187 - loss: 2.9071 - val_accuracy: 0.5696 - val_loss: 7.1786\n",
            "Epoch 18/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 318ms/step - accuracy: 0.7368 - loss: 2.6975 - val_accuracy: 0.5696 - val_loss: 7.2681\n",
            "Epoch 19/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 303ms/step - accuracy: 0.7541 - loss: 2.5001 - val_accuracy: 0.5691 - val_loss: 7.3480\n",
            "Epoch 20/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 318ms/step - accuracy: 0.7712 - loss: 2.3161 - val_accuracy: 0.5683 - val_loss: 7.4219\n",
            "Epoch 21/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 303ms/step - accuracy: 0.7888 - loss: 2.1464 - val_accuracy: 0.5661 - val_loss: 7.5292\n",
            "Epoch 22/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 302ms/step - accuracy: 0.8031 - loss: 1.9970 - val_accuracy: 0.5665 - val_loss: 7.6246\n",
            "Epoch 23/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 319ms/step - accuracy: 0.8182 - loss: 1.8463 - val_accuracy: 0.5669 - val_loss: 7.6994\n",
            "Epoch 24/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 316ms/step - accuracy: 0.8328 - loss: 1.7024 - val_accuracy: 0.5682 - val_loss: 7.8071\n",
            "Epoch 25/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 317ms/step - accuracy: 0.8463 - loss: 1.5821 - val_accuracy: 0.5675 - val_loss: 7.8956\n",
            "Epoch 26/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 316ms/step - accuracy: 0.8616 - loss: 1.4404 - val_accuracy: 0.5652 - val_loss: 7.9879\n",
            "Epoch 27/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 315ms/step - accuracy: 0.8736 - loss: 1.3242 - val_accuracy: 0.5682 - val_loss: 8.0817\n",
            "Epoch 28/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 315ms/step - accuracy: 0.8843 - loss: 1.2234 - val_accuracy: 0.5657 - val_loss: 8.1821\n",
            "Epoch 29/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 315ms/step - accuracy: 0.8976 - loss: 1.1145 - val_accuracy: 0.5653 - val_loss: 8.2792\n",
            "Epoch 30/30\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 300ms/step - accuracy: 0.9081 - loss: 1.0179 - val_accuracy: 0.5643 - val_loss: 8.3719\n",
            "Training time: 1196.70 seconds\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
            "Inference time per sample: 0.3642 seconds\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
            "BLEU Score on Test Set: 0.3426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Positional Encoding\n",
        "def get_positional_encoding(max_len, dm):\n",
        "    pos = np.arange(max_len)[:, np.newaxis]\n",
        "    i = np.arange(dm)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(dm))\n",
        "    angle_rads = pos * angle_rates\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
        "\n",
        "# Multi-head Attention block\n",
        "# Multi-head Attention block\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None): # Set a default value for training\n",
        "        attn_output = self.att(inputs, inputs, attention_mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Define Encoder\n",
        "def build_encoder(vocab_size, max_len, embed_dim, num_heads, ff_dim):\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    x = Embedding(vocab_size, embed_dim)(inputs)\n",
        "    x += get_positional_encoding(max_len, embed_dim)\n",
        "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "    return Model(inputs, x, name='Encoder')\n",
        "\n",
        "# Define Decoder\n",
        "def build_decoder(vocab_size, max_len, embed_dim, num_heads, ff_dim):\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    enc_output = Input(shape=(None, embed_dim))\n",
        "    x = Embedding(vocab_size, embed_dim)(inputs)\n",
        "    x += get_positional_encoding(max_len, embed_dim)\n",
        "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "    x = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, enc_output)\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(x)\n",
        "    return Model([inputs, enc_output], outputs, name='Decoder')\n",
        "\n",
        "# Hyperparameters\n",
        "embed_dim = 256\n",
        "num_heads = 4\n",
        "ff_dim = 512\n",
        "max_text_len = 400\n",
        "max_title_len = 20\n",
        "vocab_size = 20000\n",
        "\n",
        "# Instantiate encoder and decoder\n",
        "encoder = build_encoder(vocab_size, max_text_len, embed_dim, num_heads, ff_dim)\n",
        "decoder = build_decoder(vocab_size, max_title_len - 1, embed_dim, num_heads, ff_dim)\n",
        "\n",
        "# Define final model\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "decoder_inputs = Input(shape=(max_title_len - 1,))\n",
        "enc_out = encoder(encoder_inputs)\n",
        "dec_out = decoder([decoder_inputs, enc_out])\n",
        "model_transformer = Model([encoder_inputs, decoder_inputs], dec_out)\n",
        "model_transformer.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_transformer.summary()\n",
        "\n",
        "# For training\n",
        "model_transformer.fit([train_text_seq, decoder_input_seq], decoder_target_seq, batch_size=64, epochs=20, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qV4omimFzWKH",
        "outputId": "95444b52-b2d8-4262-c6fd-39922b028e06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_8       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m6,435,840\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m20000\u001b[0m) │ \u001b[38;5;34m12,628,256\u001b[0m │ input_layer_8[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ Encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_8       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,435,840</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">12,628,256</span> │ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ Encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,064,096\u001b[0m (72.72 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,064,096</span> (72.72 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,064,096\u001b[0m (72.72 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,064,096</span> (72.72 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 318ms/step - accuracy: 0.5127 - loss: 5.2826 - val_accuracy: 0.5275 - val_loss: 3.9185\n",
            "Epoch 2/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 232ms/step - accuracy: 0.5285 - loss: 3.8755 - val_accuracy: 0.5275 - val_loss: 3.9383\n",
            "Epoch 3/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 229ms/step - accuracy: 0.5283 - loss: 3.8345 - val_accuracy: 0.5275 - val_loss: 3.9516\n",
            "Epoch 4/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 229ms/step - accuracy: 0.5288 - loss: 3.8202 - val_accuracy: 0.5275 - val_loss: 3.9600\n",
            "Epoch 5/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 222ms/step - accuracy: 0.5269 - loss: 3.8315 - val_accuracy: 0.5275 - val_loss: 3.9682\n",
            "Epoch 6/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 228ms/step - accuracy: 0.5292 - loss: 3.8136 - val_accuracy: 0.5275 - val_loss: 3.9735\n",
            "Epoch 7/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 219ms/step - accuracy: 0.5284 - loss: 3.8198 - val_accuracy: 0.5275 - val_loss: 3.9800\n",
            "Epoch 8/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 232ms/step - accuracy: 0.5277 - loss: 3.8155 - val_accuracy: 0.5275 - val_loss: 3.9812\n",
            "Epoch 9/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 220ms/step - accuracy: 0.5290 - loss: 3.8051 - val_accuracy: 0.5275 - val_loss: 3.9814\n",
            "Epoch 10/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 233ms/step - accuracy: 0.5282 - loss: 3.7956 - val_accuracy: 0.5275 - val_loss: 3.9604\n",
            "Epoch 11/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 231ms/step - accuracy: 0.5292 - loss: 3.7562 - val_accuracy: 0.5275 - val_loss: 3.9608\n",
            "Epoch 12/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 228ms/step - accuracy: 0.5281 - loss: 3.7513 - val_accuracy: 0.5275 - val_loss: 3.9597\n",
            "Epoch 13/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 229ms/step - accuracy: 0.5291 - loss: 3.7329 - val_accuracy: 0.5275 - val_loss: 3.9606\n",
            "Epoch 14/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 217ms/step - accuracy: 0.5283 - loss: 3.7312 - val_accuracy: 0.5275 - val_loss: 3.9581\n",
            "Epoch 15/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 223ms/step - accuracy: 0.5280 - loss: 3.7188 - val_accuracy: 0.5275 - val_loss: 3.9553\n",
            "Epoch 16/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 225ms/step - accuracy: 0.5274 - loss: 3.6979 - val_accuracy: 0.5275 - val_loss: 3.9486\n",
            "Epoch 17/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 226ms/step - accuracy: 0.5292 - loss: 3.6702 - val_accuracy: 0.5275 - val_loss: 3.9494\n",
            "Epoch 18/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 227ms/step - accuracy: 0.5281 - loss: 3.6548 - val_accuracy: 0.5275 - val_loss: 3.9529\n",
            "Epoch 19/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 226ms/step - accuracy: 0.5291 - loss: 3.6404 - val_accuracy: 0.5276 - val_loss: 3.9571\n",
            "Epoch 20/20\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 226ms/step - accuracy: 0.5291 - loss: 3.6272 - val_accuracy: 0.5275 - val_loss: 3.9662\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e5a2a35cd10>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Assuming other necessary imports like numpy are already present\n",
        "\n",
        "# --- Model Definition: Encoder-Decoder with Attention ---\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,), name='encoder_inputs')\n",
        "# Ensure embedding_dim and vocab_size are defined earlier in your notebook\n",
        "enc_emb = Embedding(vocab_size, embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
        "# Encoder LSTM outputs sequences for attention and states for decoder initialization\n",
        "# Use lstm_units here as defined in your code\n",
        "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "# Decoder input is the target sequence shifted by one position\n",
        "decoder_inputs = Input(shape=(max_title_len - 1,), name='decoder_inputs')\n",
        "# Ensure max_title_len is defined earlier\n",
        "# Decoder embedding layer\n",
        "# Use the same embedding_dim and vocab_size\n",
        "dec_emb_layer = Embedding(vocab_size, embedding_dim, name='decoder_embedding')\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM, returns sequences for attention and states for inference\n",
        "# Use lstm_units here as defined in your code\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "# Attention Layer\n",
        "# Computes attention weights and applies them\n",
        "attention_layer = Attention(name='attention_layer')\n",
        "attention_output = attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenate decoder outputs and attention output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_output])\n",
        "\n",
        "# Dense layer to predict the next token\n",
        "# Use vocab_size for the output layer size\n",
        "decoder_dense = Dense(vocab_size, activation='softmax', name='decoder_output_dense')\n",
        "final_decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the final model\n",
        "model_with_attention = Model([encoder_inputs, decoder_inputs], final_decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "# Using Adam optimizer and sparse categorical crossentropy loss\n",
        "model_with_attention.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(\"--- Model with Attention Summary ---\")\n",
        "model_with_attention.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "aYkv-X-67z4O",
        "outputId": "262472d0-43b2-452e-ec8b-52f10069f928"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model with Attention Summary ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m5,120,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m5,120,000\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m,      │  \u001b[38;5;34m1,574,912\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m512\u001b[0m), │  \u001b[38;5;34m1,574,912\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concat_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention_layer[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output_den… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m20000\u001b[0m) │ \u001b[38;5;34m20,500,000\u001b[0m │ concat_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ encoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ decoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concat_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output_den… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20,500,000</span> │ concat_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,889,824\u001b[0m (129.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,889,824</span> (129.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,889,824\u001b[0m (129.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,889,824</span> (129.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_attention.fit(\n",
        "    [train_text_seq, decoder_input_seq],\n",
        "    decoder_target_seq,\n",
        "    batch_size=64,\n",
        "    epochs=15,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz6v26Ms72vl",
        "outputId": "08117b7d-5bdc-4fae-ba5b-ee41874349ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 323ms/step - accuracy: 0.5189 - loss: 4.7317 - val_accuracy: 0.5433 - val_loss: 3.7524\n",
            "Epoch 2/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 320ms/step - accuracy: 0.5470 - loss: 3.5872 - val_accuracy: 0.5513 - val_loss: 3.6335\n",
            "Epoch 3/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 321ms/step - accuracy: 0.5541 - loss: 3.3779 - val_accuracy: 0.5564 - val_loss: 3.5502\n",
            "Epoch 4/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 318ms/step - accuracy: 0.5649 - loss: 3.1280 - val_accuracy: 0.5649 - val_loss: 3.4761\n",
            "Epoch 5/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 324ms/step - accuracy: 0.5785 - loss: 2.8685 - val_accuracy: 0.5730 - val_loss: 3.4050\n",
            "Epoch 6/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 321ms/step - accuracy: 0.5922 - loss: 2.5752 - val_accuracy: 0.5738 - val_loss: 3.3848\n",
            "Epoch 7/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 319ms/step - accuracy: 0.6084 - loss: 2.2993 - val_accuracy: 0.5796 - val_loss: 3.3981\n",
            "Epoch 8/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 325ms/step - accuracy: 0.6315 - loss: 2.0527 - val_accuracy: 0.5811 - val_loss: 3.4177\n",
            "Epoch 9/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - accuracy: 0.6667 - loss: 1.7979 - val_accuracy: 0.5831 - val_loss: 3.4595\n",
            "Epoch 10/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 317ms/step - accuracy: 0.7002 - loss: 1.5692 - val_accuracy: 0.5824 - val_loss: 3.4963\n",
            "Epoch 11/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 325ms/step - accuracy: 0.7342 - loss: 1.3568 - val_accuracy: 0.5834 - val_loss: 3.5431\n",
            "Epoch 12/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 319ms/step - accuracy: 0.7657 - loss: 1.1786 - val_accuracy: 0.5850 - val_loss: 3.6088\n",
            "Epoch 13/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 312ms/step - accuracy: 0.7933 - loss: 1.0277 - val_accuracy: 0.5857 - val_loss: 3.6691\n",
            "Epoch 14/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 310ms/step - accuracy: 0.8204 - loss: 0.8852 - val_accuracy: 0.5846 - val_loss: 3.7262\n",
            "Epoch 15/15\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 311ms/step - accuracy: 0.8468 - loss: 0.7624 - val_accuracy: 0.5856 - val_loss: 3.7756\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e5a701b4290>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "# Extract texts and titles\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "test_titles = test_df['title'].astype(str).tolist()\n",
        "\n",
        "# Preprocess test data for both models\n",
        "test_text_seq = text_tokenizer.texts_to_sequences(test_texts)\n",
        "test_text_seq = pad_sequences(test_text_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# Prepare decoder input sequences for model_no_attention\n",
        "test_title_seq = title_tokenizer.texts_to_sequences(test_titles)\n",
        "test_title_seq = pad_sequences(test_title_seq, maxlen=max_title_len, padding='post')\n",
        "decoder_input_test_seq = test_title_seq[:, :-1]\n",
        "\n",
        "# Evaluate model_no_attention\n",
        "loss_no_att, acc_no_att = model_no_attention.evaluate([test_text_seq, decoder_input_test_seq], test_title_seq[:, 1:], verbose=0)\n",
        "print(f\"Model Without Attention - Loss: {loss_no_att}, Accuracy: {acc_no_att}\")\n",
        "\n",
        "# Prepare target sequences for model_self_attention\n",
        "test_title_seq_flat = np.array([seq[0] if len(seq) > 0 else 0 for seq in test_title_seq])\n",
        "\n",
        "# Evaluate model_self_attention\n",
        "# Evaluate model_transformer (renamed from model_self_attention)\n",
        "loss_self_att, acc_self_att = model_transformer.evaluate([test_text_seq, decoder_input_test_seq], test_title_seq[:, 1:], verbose=0) # Use model_transformer and correct input format\n",
        "print(f\"Model With Self-Attention - Loss: {loss_self_att}, Accuracy: {acc_self_att}\")\n",
        "\n",
        "\n",
        "# Evaluate model_with_attention\n",
        "loss_with_att, acc_with_att = model_with_attention.evaluate([test_text_seq, decoder_input_test_seq], test_title_seq[:, 1:], verbose=0)\n",
        "print(f\"Model With Attention - Loss: {loss_with_att}, Accuracy: {acc_with_att}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL-fkXPD5anf",
        "outputId": "cfa335cf-0abd-4b13-f7b0-dfa297542aec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Without Attention - Loss: 2.8694217205047607, Accuracy: 0.7836257815361023\n",
            "Model With Self-Attention - Loss: 3.8176803588867188, Accuracy: 0.5321637988090515\n",
            "Model With Attention - Loss: 1.6158041954040527, Accuracy: 0.7426900267601013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_headline(text_input, model, max_title_len=20):\n",
        "    # Tokenize and pad the input text\n",
        "    seq = text_tokenizer.texts_to_sequences([text_input])\n",
        "    seq = pad_sequences(seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "    # Start the decoder with the <start> token\n",
        "    start_token = title_tokenizer.word_index.get('<start>', 1)  # Use 1 if <start> not defined\n",
        "    end_token = title_tokenizer.word_index.get('<end>', 2)      # Use 2 if <end> not defined\n",
        "    output_seq = [start_token]\n",
        "\n",
        "    for i in range(max_title_len - 1):\n",
        "        # Pad decoder input\n",
        "        dec_input = pad_sequences([output_seq], maxlen=max_title_len - 1, padding='post')\n",
        "        # Predict next word\n",
        "        preds = model.predict([seq, dec_input], verbose=0)\n",
        "        next_word_id = np.argmax(preds[0, i])\n",
        "        output_seq.append(next_word_id)\n",
        "\n",
        "        if next_word_id == end_token:\n",
        "            break\n",
        "\n",
        "    # Convert token IDs back to words\n",
        "    inverse_title_index = {v: k for k, v in title_tokenizer.word_index.items()}\n",
        "    words = [inverse_title_index.get(i, '') for i in output_seq if i not in [0, start_token, end_token]]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Example usage\n",
        "custom_text = \" U.S. President Donald Trump faces an uphill battle to overcome a federal judgeâ€™s temporary hold on his travel ban on seven mainly Muslim countries, but the outcome of a ruling on the executive orderâ€™s ultimate legality is less certain.  Any appeals of decisions by U.S. District Court Judge James Robart in Seattle face a regional court dominated by liberal-leaning judges who might not be sympathetic to Trumpâ€™s rationale for the ban, and a currently shorthanded Supreme Court split 4-4 between liberals and conservatives. The temporary restraining order Robart issued on Friday in Seattle, which applies nationwide, gives him time to consider the case in more detail, but also sends a signal that he is likely to impose a more permanent injunction.    The Trump administration has appealed that order. The San Francisco-based 9th U.S. Circuit Court of Appeals said late on Saturday that it would not decide whether to lift the judgeâ€™s ruling, as requested by the U.S. government, until it receives briefs from both sides, with the administrationâ€™s filing due on Monday. Appeals courts are generally leery of upending the status quo, which in this case - for now - is the suspension of the ban.  The upheaval prompted by the new Republican administrationâ€™s initial announcement of the ban on Jan. 27, with travelers detained at airports upon entering the country, would potentially be kickstarted again if Robartâ€™s stay was lifted. The appeals court might also take into account the fact that there are several other cases around the country challenging the ban. If it were to overturn the district courtâ€™s decision, another judge somewhere else in the United States could impose a new order, setting off a new cascade of court filings.  If the appeals court upholds the order, the administration could immediately ask the U.S. Supreme Court to intervene. But the high court is generally reluctant to get involved in cases at a preliminary stage, legal experts said. The high court is short one justice, as it has been for a year, leaving it split between liberals and conservatives. Any emergency request by the administration would need five votes to be granted, meaning at least one of the liberals would have to vote in favor.  â€œI think the courtâ€™s going to feel every reason to stay on the sidelines as long as possible,â€ said Steve Vladeck, a professor at the University of Texas School of Law. Trump last week nominated a conservative appeals court judge, Neil Gorsuch, to fill the vacancy, but he will not be sitting on the Supreme Court for at least two months. Gorsuchâ€™s vote, if he is confirmed by the U.S. Senate, could come into play if the case were to reach the court at a later stage of the litigation. Once the case proceeds past the injunction stage of the litigation and onto the merits of whether the order is legally sound, legal experts differ over how strong the governmentâ€™s case would be.  Richard Primus, a professor of constitutional law at the University of Michigan Law School, said the administration could struggle to convince courts that the ban was justified by national security concerns.  The Supreme Court has previously rejected the idea that the government does not need to offer a basis for its actions in the national security context, including the landmark 1971 Pentagon Papers case, in which the administration of President Richard Nixon tried unsuccessfully to prevent the press from publishing information about United States policy toward Vietnam. â€œThe governmentâ€™s argument so far in support of the order is pretty weak,â€ Primus said. Jonathan Adler, a professor at Case Western Reserve University School of Law, said the administration has legal precedent on its side, with the courts generally deferential to executive action on immigration.  However, he said it is unusual for the courts to be asked to endorse â€œa policy that appears to have been adopted in as kind of haphazard and arbitrary way as this one appears to have been.\"\n",
        "generated_headline = generate_headline(custom_text, model_with_attention)\n",
        "print(\"Generated Headline:\", generated_headline)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH1JPFA8BbG9",
        "outputId": "ab1e1d42-9a5e-4ffe-fc87-e2819e3bcbcc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Headline: faces uphill battle against trump supreme court nominee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_headline_no_attention(text_input, model, max_title_len=20):\n",
        "    # Tokenize and pad the input text\n",
        "    seq = text_tokenizer.texts_to_sequences([text_input])\n",
        "    seq = pad_sequences(seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "    # Start the decoder with the <start> token\n",
        "    start_token = title_tokenizer.word_index.get('<start>', 1)  # Use 1 if <start> not defined\n",
        "    end_token = title_tokenizer.word_index.get('<end>', 2)      # Use 2 if <end> not defined\n",
        "    output_seq = [start_token]\n",
        "\n",
        "    for i in range(max_title_len - 1):\n",
        "        # Pad decoder input\n",
        "        dec_input = pad_sequences([output_seq], maxlen=max_title_len - 1, padding='post')\n",
        "        # Predict next word\n",
        "        preds = model.predict([seq, dec_input], verbose=0)\n",
        "        next_word_id = np.argmax(preds[0, i])\n",
        "        output_seq.append(next_word_id)\n",
        "\n",
        "        if next_word_id == end_token:\n",
        "            break\n",
        "\n",
        "    # Convert token IDs back to words\n",
        "    inverse_title_index = {v: k for k, v in title_tokenizer.word_index.items()}\n",
        "    words = [inverse_title_index.get(i, '') for i in output_seq if i not in [0, start_token, end_token]]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Example usage\n",
        "custom_text = \"a usfunded famine survey said on tuesday that thousands of yemenis could die daily if a saudiled military coalition does not lift its blockade on the country s key ports the warning came a day after the international committee of the red cross icrc said  million people in yemen s crowded cities had no access to clean water raising the risk that a cholera epidemic will spread using the internationally recognized ipc  scale for classifying food security the famine early warning systems network fews net said that even before the current blockade  million people were in  crisis  ipc phase  or worse  therefore a prolonged closure of key ports risks an unprecedented deterioration in food security to famine ipc phase  across large areas of the country  it said  it said famine is likely in many areas within three or four months if ports remain closed with some less accessible areas at even greater risk in such a scenario shortages of food and fuel would drive up prices and a lack of medical supplies would exacerbate lifethreatening diseases  thousands of deaths would occur each day due to the lack of food and disease outbreaks  said fews net which is funded by the us agency for international development  famine remained likely even if the southern port of aden is open its report said adding that all yemeni ports should be reopened to essential imports the united nations has said the saudiled coalition must allow aid in through hodeidah port controlled by the houthis the saudis  enemies in the war in yemen  un humanitarian agencies have issued dire warnings about the impact of the blockade although un officials have declined to directly criticize saudi arabia jan egeland head of the norwegian refugee council and a former un aid chief was blunt in his criticism however  us uk  other allies of saudi has only weeks to avoid being complicit in a famine of biblical proportions lift the blockade now  he wrote in a tweet   last year saudi arabia was accused by then un secretary general ban kimoon of exerting  unacceptable  undue pressure to keep itself off a blacklist of countries that kill children in conflict after sources told reuters that riyadh had threatened to cut some un funding saudi arabia denied this.\"\n",
        "generated_headline = generate_headline_no_attention(custom_text, model_no_attention)  # Pass model_no_attention here\n",
        "print(\"Generated Headline:\", generated_headline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJACJIW5ozr6",
        "outputId": "a0de1cfc-84ad-48b3-e603-9fffad057599"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Headline: humanitarian situation falls under return\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install nltk rouge-score"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcn25uD531Q4",
        "outputId": "881476a6-49ea-49af-d8bc-25a7b16b97d0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "la9mctmv32AK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "source": [
        "def calculate_bleu(model, inputs, targets, tokenizer):\n",
        "    predictions = model.predict(inputs)\n",
        "    predicted_ids = np.argmax(predictions, axis=-1)\n",
        "    references = [[tokenizer.index_word.get(id, '') for id in target if id != 0] for target in targets]\n",
        "    hypotheses = [[tokenizer.index_word.get(id, '') for id in pred if id != 0] for pred in predicted_ids]\n",
        "    scores = [sentence_bleu([ref], hyp) for ref, hyp in zip(references, hypotheses)]\n",
        "    return np.mean(scores)\n",
        "\n",
        "def calculate_rouge(model, inputs, targets, tokenizer):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "    predictions = model.predict(inputs)\n",
        "    predicted_ids = np.argmax(predictions, axis=-1)\n",
        "    references = [' '.join([tokenizer.index_word.get(id, '') for id in target if id != 0]) for target in targets]\n",
        "    hypotheses = [' '.join([tokenizer.index_word.get(id, '') for id in pred if id != 0]) for pred in predicted_ids]\n",
        "    scores = [scorer.score(ref, hyp)['rouge1'].fmeasure for ref, hyp in zip(references, hypotheses)]\n",
        "    return np.mean(scores)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JOghd0iI33aq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "source": [
        "# For model_no_attention\n",
        "bleu_score_no_attention = calculate_bleu(model_no_attention, [test_text_seq, test_title_seq[:, :-1]], test_title_seq[:, 1:], title_tokenizer)\n",
        "rouge_score_no_attention = calculate_rouge(model_no_attention, [test_text_seq, test_title_seq[:, :-1]], test_title_seq[:, 1:], title_tokenizer)\n",
        "\n",
        "# For model_with_attention\n",
        "bleu_score_with_attention = calculate_bleu(model_with_attention, [test_text_seq, test_title_seq[:, :-1]], test_title_seq[:, 1:], title_tokenizer)\n",
        "rouge_score_with_attention = calculate_rouge(model_with_attention, [test_text_seq, test_title_seq[:, :-1]], test_title_seq[:, 1:], title_tokenizer)\n",
        "\n",
        "print(f\"Model Without Attention - BLEU: {bleu_score_no_attention:.4f}, ROUGE-1: {rouge_score_no_attention:.4f}\")\n",
        "print(f\"Model With Attention - BLEU: {bleu_score_with_attention:.4f}, ROUGE-1: {rouge_score_with_attention:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYC2Gp1-36a2",
        "outputId": "6c1fa3a1-7ab4-4f03-a742-8ffc042d2922"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "Model Without Attention - BLEU: 0.3426, ROUGE-1: 0.6452\n",
            "Model With Attention - BLEU: 0.1242, ROUGE-1: 0.4573\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already calculated the scores as mentioned in the previous response\n",
        "\n",
        "models = ['Without Attention', 'With Attention']\n",
        "bleu_scores = [bleu_score_no_attention, bleu_score_with_attention]\n",
        "rouge_scores = [rouge_score_no_attention, rouge_score_with_attention]\n",
        "\n",
        "x = np.arange(len(models))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, bleu_scores, width, label='BLEU')\n",
        "rects2 = ax.bar(x + width/2, rouge_scores, width, label='ROUGE-1')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('BLEU and ROUGE-1 Scores for Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "ax.bar_label(rects1, labels=[f'{score:.4f}' for score in bleu_scores], label_type='center')\n",
        "ax.bar_label(rects2, labels=[f'{score:.4f}' for score in rouge_scores], label_type='center')\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Gu4h48j44IHB",
        "outputId": "ea7ddc6e-c291-4c6a-e30d-1308e5d85085"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWFdJREFUeJzt3XlYVGX/BvB7ZpiFHZFVRFAxFTcMXHsTSdzS1DctU0sk87XU1NBKW1wrKktpMU2TNE0zc3vdcCFJS15NzUxT3LdkFRk22Wae3x/8ODrOoIDI4PH+XNdcNc95zjnfcwaON89ZRiGEECAiIiKiB57S2gUQERERUfVgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiydKlS6FQKHDhwgVrl0IylZqaikGDBqFu3bpQKBSIiYmxdkn3xYgRI+Dv71+lebt27YquXbtWaz308GCwI9krCyu3vjw8PBAWFoZt27aZ9VcoFBg3btwdl9m1a1ezZZa9mjVrJvWbMWMGFAoFMjIyLC6nZcuWD+QBvGy7yl5qtRr+/v4YP348srKyLM5TXFyMzz//HO3atYOjoyMcHBzQrl07fP755yguLjbrf6fP4aeffoJCoUBCQoLZtL179+LZZ5+Fj48PNBoNnJ2d0aFDB8yaNQupqakmfSv6OZYnOTkZU6ZMQVhYGBwdHcut6U42bdqE0NBQeHh4wM7ODo0aNcKzzz6LuLi4Si3nQfHaa69h+/btmDp1KpYvX45evXrd1/WVfZ4vvfSSxelvv/221Ke831OiB4mNtQsgqimzZs1Cw4YNIYRAamoqli5diieffBKbNm1C3759K728+vXrIzo62qzd2dm5Osp9ICxYsAAODg7Iy8tDfHw8vvjiCxw+fBi//vqrSb+8vDz06dMHv/zyC/r27YsRI0ZAqVQiLi4OEyZMwLp167BlyxbY29vfUz3Tpk3D7Nmz0ahRI4wYMQKNGjVCQUEBDh06hE8//RTLli3D2bNnTea5l88xKSkJH330EZo0aYJWrVohMTGxUvV+8skneP311xEaGoqpU6fCzs4OZ86cwa5du/DDDz/c99BjDT///DP69++PyZMn19g6dTod1q5di6+++goajcZk2qpVq6DT6VBQUFBj9RDdTwx29NDo3bs3QkJCpPcjR46Ep6cnVq1aVaVg5+zsjOeff746S3zgDBo0CG5ubgCA0aNH47nnnsPq1atx4MABtG/fXuoXFRWFX375BV988YXJKNwrr7yC+fPnY9y4cZg8eTIWLFhQ5VpWr16N2bNn49lnn8Xy5cvN/gGfN28e5s2bZzbfvXyOwcHBuHbtGlxdXfHTTz/hmWeeqfC8JSUlmD17Nrp3744dO3aYTU9LS6tSTVVhNBpRVFQEnU5339eVlpYGFxeXalteQUEBNBoNlMryT0D16tUL//3vf7Ft2zb0799fat+3bx/Onz+PgQMHYu3atdVWE5E18VQsPbRcXFxga2sLG5va//fNt99+iyeeeAIeHh7QarUIDAy0GIL8/f3Rt29f/Prrr2jfvj10Oh0aNWqE7777zqzv8ePH8cQTT8DW1hb169fHe++9B6PReE91Pv744wBgMip25coVLFmyBE888YTFU6tjx45FWFgYvvnmG1y5cqXK6542bRrc3NywZMkSs1AHlAa4GTNmVHn5ljg6OsLV1bVK82ZkZCA7OxuPPfaYxekeHh4m7wsKCjBjxgw88sgj0Ol08Pb2xtNPP22yr/Py8jBp0iT4+vpCq9WiadOm+OSTTyCEMFlW2Wnu77//Hi1atIBWq5VO/f7zzz948cUX4enpCa1WixYtWiA2Ntasvi+++AItWrSAnZ0d6tSpg5CQEKxcubLc7S27JEIIgfnz50unP8ucO3cOzzzzDFxdXWFnZ4eOHTtiy5YtJstISEiAQqHADz/8gHfeeQc+Pj6ws7NDdnZ2uesFAB8fH3Tp0sWsvu+//x6tWrVCy5YtLc63Zs0aBAcHw9bWFm5ubnj++efxzz//mPXbsGEDWrZsCZ1Oh5YtW2L9+vUWl2c0GhETE4MWLVpAp9PB09MTo0ePxvXr1+9YP1D5/U0Pr9r/LxpRNdHr9cjIyIAQAmlpafjiiy+Qm5tb5dEag8Fg8ZocW1vbez6leLsFCxagRYsW6NevH2xsbLBp0yaMGTMGRqMRY8eONel75swZDBo0CCNHjkRERARiY2MxYsQIBAcHo0WLFgCAlJQUhIWFoaSkBFOmTIG9vT0WLVoEW1vbe6qz7KaLOnXqSG3btm2DwWDA8OHDy51v+PDh2L17N+Li4sq9FupOTp06hVOnTuGll16Cg4NDpeatyc/xVh4eHrC1tcWmTZvw6quv3jEgGgwG9O3bF/Hx8XjuuecwYcIE5OTkYOfOnTh27BgaN24MIQT69euH3bt3Y+TIkQgKCsL27dvx+uuv459//jEbrfz555/x448/Yty4cXBzc4O/vz9SU1PRsWNHKfi5u7tj27ZtGDlyJLKzszFx4kQAwOLFizF+/HgMGjQIEyZMQEFBAY4ePYr9+/dj6NChFrehS5cuWL58OV544QV0797d5OchNTUVnTt3Rn5+PsaPH4+6deti2bJl6NevH3766Sf8+9//NlnW7NmzodFoMHnyZBQWFloM8rcbOnQoJkyYgNzcXDg4OKCkpARr1qxBVFSUxdOwS5cuRWRkJNq1a4fo6Gikpqbis88+w2+//YY//vhDGnXcsWMHBg4ciMDAQERHR+PatWuIjIxE/fr1zZY5evRoabnjx4/H+fPn8eWXX+KPP/7Ab7/9BrVabbH2quxveogJIpn79ttvBQCzl1arFUuXLjXrD0CMHTv2jssMDQ21uEwAYvTo0VK/6dOnCwAiPT3d4nJatGghQkND77oN+fn5Zm09e/YUjRo1Mmnz8/MTAMSePXuktrS0NKHVasWkSZOktokTJwoAYv/+/Sb9nJ2dBQBx/vz5O9ZTtl1JSUkiPT1dXLhwQcTGxgpbW1vh7u4u8vLyzNb1xx9/lLu8w4cPCwAiKipKarvT57BmzRoBQOzevVsIIcTGjRsFABETE2PSz2g0ivT0dJNXcXGxNL2in2NF3F5TRUybNk0AEPb29qJ3797i/fffF4cOHTLrFxsbKwCIuXPnmk0zGo1CCCE2bNggAIj33nvPZPqgQYOEQqEQZ86ckdoACKVSKY4fP27Sd+TIkcLb21tkZGSYtD/33HPC2dlZ+jns37+/aNGiRYW381aWPteyn5G9e/dKbTk5OaJhw4bC399fGAwGIYQQu3fvFgBEo0aNLP5O3Gl9mZmZQqPRiOXLlwshhNiyZYtQKBTiwoULZr+nRUVFwsPDQ7Rs2VLcuHFDWtbmzZsFADFt2jSpLSgoSHh7e4usrCypbceOHQKA8PPzk9r27t0rAIjvv//epL64uDiz9tDQUJPjwr3sb3r48FQsPTTmz5+PnTt3YufOnVixYgXCwsLw0ksvYd26dVVanr+/v7S8W19loxrV6daRtLKRx9DQUJw7dw56vd6kb2BgoHRKFADc3d3RtGlTnDt3TmrbunUrOnbsaHIdnLu7O4YNG1apupo2bQp3d3f4+/vjxRdfREBAALZt2wY7OzupT05ODoDS05blKZt2t1Nq5Smb7/bROr1eD3d3d5PXkSNHTPrU5Od4u5kzZ2LlypVo27Yttm/fjrfffhvBwcF49NFHceLECanf2rVr4ebmhldffdVsGWWnM7du3QqVSoXx48ebTJ80aRKEEGZ3gIeGhiIwMFB6L4TA2rVr8dRTT0EIgYyMDOnVs2dP6PV6HD58GEDpZQxXrlzB77//Xi37YevWrWjfvj3+9a9/SW0ODg74z3/+gwsXLuDvv/826R8REVHp0eU6deqgV69eWLVqFQBg5cqV6Ny5M/z8/Mz6Hjx4EGlpaRgzZozJdYd9+vRBs2bNpFPEycnJOHLkCCIiIkxutunevbvJvgVKT+s6Ozuje/fuJvs2ODgYDg4O2L17d7m1V/f+JnnjqVh6aLRv397k5okhQ4agbdu2GDduHPr27Vuh0zm3sre3R3h4+D3Xdet1RuX57bffMH36dCQmJiI/P99kml6vN/lHpUGDBmbz16lTx+Q6nosXL6JDhw5m/Zo2bVqZ0rF27Vo4OTkhPT0dn3/+Oc6fP2/2D25ZaCsLeJZUJPxZUrbvyubLzc01me7g4ICdO3cCKD1lNmfOHLNl3O1zLCoqQmZmpkmbu7s7VCpVpWotz5AhQzBkyBBkZ2dj//79WLp0KVauXImnnnoKx44dg06nw9mzZ9G0adM7Xg968eJF1KtXz2wfNm/eXJp+q4YNG5q8T09PR1ZWFhYtWoRFixZZXEfZDR1vvvkmdu3ahfbt2yMgIAA9evTA0KFDy71e8G7K+3m8tfZbr4O7vfaKGjp0KF544QVcunQJGzZswMcff1xuPYDl34dmzZpJd32X9WvSpIlZv6ZNm0pBGABOnz4NvV5vdu1kmTvdLFPd+5vkjcGOHlpKpRJhYWH47LPPcPr0aen6s+pU9tf+jRs3LE7Pz8+/652IZ8+eRbdu3dCsWTPMnTsXvr6+0Gg02Lp1K+bNm2d2w0N5gUPcdgF9dejSpYt0V+xTTz2FVq1aYdiwYTh06JB0l2LZP85Hjx5FUFCQxeUcPXoUAExGObRa7R33G3Bz/5Y9c+7YsWMm/WxsbKTQVtUbM/bt24ewsDCTtvPnz1f54bPlcXJyQvfu3dG9e3eo1WosW7YM+/fvR2hoaLWup8ztAbzs5+j5559HRESExXlat24NoPQzTUpKwubNmxEXFyc9SmTatGmYOXPmfan3VlW9FrRfv37QarWIiIhAYWEhnn322WqurHxGoxEeHh74/vvvLU53d3cvd15r7296sDDY0UOtpKQEgPlIT3UpO82TlJQEX19fk2n5+fm4fPkyevToccdlbNq0CYWFhfjvf/9rMhp3p1M3Fanr9OnTZu1JSUlVXqaDgwOmT5+OyMhI/Pjjj3juuecAlD5mRqVSYfny5eXeQPHdd9/BxsbG5Lltfn5+5dZT1l62f5s2bYomTZpgw4YNiImJqdabHtq0aSON+pXx8vKqtuVbEhISgmXLliE5ORkA0LhxY+zfvx/FxcXlXmDv5+eHXbt2IScnx2TU7uTJk9L0O3F3d4ejoyMMBkOFRqLt7e0xePBgDB48GEVFRXj66afx/vvvY+rUqZV+bEp5n3VFa68oW1tbDBgwACtWrEDv3r2lP0os1QOU/pw98cQTJtOSkpKk6WX/rcjvUuPGjbFr1y489thjVQqm1bm/Sd54jR09tIqLi7Fjxw5oNBppVKm6devWDRqNBgsWLDAbWVu0aBFKSkrQu3fvOy6jbATu1hE3vV6Pb7/9tsp1Pfnkk/jf//6HAwcOSG3p6enljiZU1LBhw1C/fn189NFHUpuvry8iIyOxa9cui49oWbhwIX7++WeMHDnS5E7CshoPHTpk0j8rKwvff/89goKCTALWjBkzkJGRgVGjRln8JouqjljWqVMH4eHhJq/q+Ic0Pz+/3Acal10PV3YqcODAgcjIyMCXX35p1rdsu5588kkYDAazPvPmzYNCoajQz1nZ89xuH/kESn8+yly7ds1kmkajQWBgIIQQFvf93Tz55JM4cOCAyf7Iy8vDokWL4O/vb3a92r2YPHkypk+fjnfffbfcPiEhIfDw8MDChQtRWFgotW/btg0nTpxAnz59AADe3t4ICgrCsmXLTK513blzp9l1gc8++ywMBgNmz55ttr6SkpJyv7EFqP79TfLGETt6aGzbtk0aAUhLS8PKlStx+vRpTJkyBU5OTiZ9Dx48iPfee89sGV27dpUu8Nbr9VixYoXFdZU9QsXDwwPTpk3DO++8gy5duqBfv36ws7PDvn37sGrVKvTo0QNPPfXUHevu0aMHNBoNnnrqKYwePRq5ublYvHgxPDw8pBGdynrjjTekr3OaMGGC9LgTPz8/6bRoVajVakyYMAGvv/464uLipBG4efPm4eTJkxgzZoxJ+/bt27Fx40aEhobi008/NVnWlClTsGbNGnTp0gWjR49Gs2bNcPXqVSxduhTJyclmwXbo0KE4duwYoqOjceDAATz33HNo2LAh8vLycOzYMaxatQqOjo4mj2IBKvY53knZz8nx48cBAMuXL5euwXrnnXfKnS8/Px+dO3dGx44d0atXL/j6+iIrKwsbNmzA3r17MWDAALRt2xZA6eNgvvvuO0RFReHAgQN4/PHHkZeXh127dmHMmDHo378/nnrqKYSFheHtt9/GhQsX0KZNG+zYsQMbN27ExIkT0bhx47tuy4cffojdu3ejQ4cOGDVqFAIDA5GZmYnDhw9j165d0rWGPXr0gJeXFx577DF4enrixIkT+PLLL9GnT59KXycJlH7Wq1atQu/evTF+/Hi4urpi2bJlOH/+PNauXXvHhw9XVps2bdCmTZs79lGr1fjoo48QGRmJ0NBQDBkyRHrcib+/P1577TWpb3R0NPr06YN//etfePHFF5GZmSk9c+7WMwGhoaEYPXo0oqOjceTIEfTo0QNqtRqnT5/GmjVr8Nlnn2HQoEEW66nu/U0yZ7X7cYlqiKXHneh0OhEUFCQWLFggPS6izO19b33Nnj1bCHHnx2RY+rVasWKF6Nixo7C3txdarVY0a9ZMzJw5UxQUFFRoG/773/+K1q1bC51OJ/z9/cVHH30kPQLj1keT+Pn5iT59+pjNf/vjE4QQ4ujRoyI0NFTodDrh4+MjZs+eLZYsWVKpx51YeoyLXq8Xzs7OZusrLCwU8+bNE8HBwcLe3l7Y2dmJRx99VMTExIiioiKL67ly5Yp46aWXhI+Pj7CxsRGurq6ib9++4n//+1+5tSUkJIhBgwYJb29voVarhZOTkwgJCRHTp08XycnJZvulMp+jJVWdv7i4WCxevFgMGDBA+Pn5Ca1WK+zs7ETbtm3FnDlzRGFhoUn//Px88fbbb4uGDRsKtVotvLy8xKBBg8TZs2elPjk5OeK1114T9erVE2q1WjRp0kTMmTPH4s94eY+SSU1NFWPHjhW+vr7Serp16yYWLVok9fn6669Fly5dRN26dYVWqxWNGzcWr7/+utDr9RXaX5bWffbsWTFo0CDh4uIidDqdaN++vdi8ebNJn7LHnaxZs+au66nItpYp7+d59erVom3btkKr1QpXV1cxbNgwceXKFbP5165dK5o3by60Wq0IDAwU69atExERESaPOymzaNEiERwcLGxtbYWjo6No1aqVeOONN8TVq1elPrf/vt7L/qaHj0KI+3BFNRERERHVOF5jR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMvHQPaDYaDTi6tWrcHR0rNCXrxMRERFZkxACOTk5qFev3l0f2P3QBburV6+afWcnERERUW13+fJlk69etOShC3ZlX79y+fJls6+RIiIiIqptsrOz4evrW6GvkHvogl3Z6VcnJycGOyIiInpgVOQSMt48QURERCQTDHZEREREMsFgR0RERCQTD901dkRERHJlMBhQXFxs7TKoktRqNVQqVbUsi8GOiIjoASeEQEpKCrKysqxdClWRi4sLvLy87vkZuwx2RERED7iyUOfh4QE7Ozs+gP8BIoRAfn4+0tLSAADe3t73tDwGOyIiogeYwWCQQl3dunWtXQ5Vga2tLQAgLS0NHh4e93RaljdPEBERPcDKrqmzs7OzciV0L8o+v3u9RpLBjoiISAZ4+vXBVl2fH4MdERERkUww2BERERHJBG+eICIikin/KVtqdH0XPuxTqf4jRozAsmXLpPeurq5o164dPv74Y7Ru3RpA6SnK9evXY8CAAWbzJyQkICwszOKyk5OT4eXlhREjRiArKwsbNmywOO/169fh4uJSqbprM47YERERkdX06tULycnJSE5ORnx8PGxsbNC3b99KLSMpKUlaRtnLw8PjPlVcu3HEjoiIiKxGq9XCy8sLAODl5YUpU6bg8ccfR3p6Otzd3Su0DA8PD1mNut0LjtgRERFRrZCbm4sVK1YgICCAz+SrIo7YERERkdVs3rwZDg4OAIC8vDx4e3tj8+bNUCorPvZUv359k/d+fn44fvx4tdb5oGCwIyIiIqsJCwvDggULAADXr1/HV199hd69e+PAgQPw8/Or0DL27t0LR0dH6b1arb4vtT4IGOzo4TXD2doV0N3M0Fu7AiK6z+zt7REQECC9/+abb+Ds7IzFixfjvffeq9AyGjZsWO41dk5OTrh48aJZe1ZWFlQqFezt7atUd23Fa+yIiIio1lAoFFAqlbhx40a1LK9p06Y4fvw4CgsLTdoPHz6Mhg0bym50jyN2REREZDWFhYVISUkBUHoq9ssvv0Rubi6eeuopqc/58+dx5MgRk/maNGki/X9aWhoKCgpMptetWxdqtRrDhg3DrFmzMHz4cLzxxhtwdnbGnj17EBMTg48//vj+bZiVMNgRERGR1cTFxcHb2xsA4OjoiGbNmmHNmjXo2rWr1CcqKspsvr1790r/37RpU7PpiYmJ6NixI1xcXLB3715MmTIF/fr1g16vR0BAAObOnYuRI0dW/wZZmUIIIaxdRE3Kzs6Gs7Mz9Ho9nJycrF0OWROvsav9eI0d0V0VFBTg/PnzaNiwIXQ6nbXLoSq60+dYmezCa+yIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgm+F2xREREclXTX51Yya8BHDFiBJYtWwYAsLGxQf369fHMM89g1qxZJl+rtXnzZsyZMweHDx+GwWBAixYtMHbsWIwYMULqk5CQgLCwMFy/fh0uLi4m6/H398fEiRMxceJEqW337t349NNPsX//fuTk5MDHxwchISEYO3YsunTpYrJMS5KTk+Hl5WVx2p49ezBnzhwcOnQIycnJWL9+PQYMGFCpfVNVHLEjIiIiq+nVqxeSk5Nx7tw5zJs3D19//TWmT58uTf/iiy/Qv39/PPbYY9i/fz+OHj2K5557Di+//DImT55cpXV+9dVX6NatG+rWrYvVq1cjKSkJ69evR+fOnfHaa6+Z9U9KSkJycrLJy8PDo9zl5+XloU2bNpg/f36V6rsXHLEjIiIiq9FqtdLIl6+vL8LDw7Fz50589NFHuHz5MiZNmoSJEyfigw8+kOaZNGkSNBoNxo8fj2eeeQYdOnSo8PouXbokjd7NnTvXZFrr1q0xfvx4s3k8PDzMRgHvpHfv3ujdu3eF+1cnjtgRERFRrXDs2DHs27cPGo0GAPDTTz+huLjY4sjc6NGj4eDggFWrVlVqHWvXrkVxcTHeeOMNi9MVCkXlC69FGOyIiIjIajZv3gwHBwfodDq0atUKaWlpeP311wEAp06dgrOzM7y9vc3m02g0aNSoEU6dOlWp9Z06dQpOTk4m18etXbsWDg4O0uuvv/4ymad+/fom01u0aFGFLa0ZPBVLREREVhMWFoYFCxYgLy8P8+bNg42NDQYOHHhf13n7qFzPnj1x5MgR/PPPP+jatSsMBoPJ9L1798LR0VF6r1arpfZbT7l+/fXXGDZs2H2s/O4Y7IiIiMhq7O3tERAQAACIjY1FmzZtsGTJEowcORKPPPII9Ho9rl69inr16pnMV1RUhLNnz0p3rTo5OQEA9Hq92fVwWVlZcHYuvUO4SZMm0Ov1SElJkUbtHBwcEBAQABsby7GoYcOGFq+xCwkJwZEjR6T3np6eld7+6mb1U7Hz58+Hv78/dDodOnTogAMHDtyxf1ZWFsaOHQtvb29otVo88sgj2Lp1aw1VS0RERPeLUqnEW2+9hXfeeQc3btzAwIEDoVar8emnn5r1XbhwIfLy8jBkyBAApYFNqVTi0KFDJv3OnTsHvV6PRx55BAAwaNAgqNVqfPTRR/dcr62tLQICAqTXraN61mLVEbvVq1cjKioKCxcuRIcOHRATE4OePXsiKSnJ4m3ERUVF6N69Ozw8PPDTTz/Bx8cHFy9erNSdKkRERFR7PfPMM3j99dcxf/58TJ48GR9//DEmTZoEnU6HF154AWq1Ghs3bsRbb72FSZMmSXfEOjo64qWXXsKkSZNgY2ODVq1a4fLly3jzzTfRsWNHdO7cGQDQoEEDfPrpp5gwYQIyMzMxYsQINGzYEJmZmVixYgUAQKVSmdSUlpaGgoICk7a6detKp2Rvl5ubizNnzkjvz58/jyNHjsDV1RUNGjSotn1liVWD3dy5czFq1ChERkYCKE3fW7ZsQWxsLKZMmWLWPzY2FpmZmdi3b5+0M/39/WuyZCIiIrqPbGxsMG7cOHz88cd45ZVXMHHiRDRq1AiffPIJPvvsM+kBxQsWLJDyQ5nPPvsMH374Id58801cvHgRXl5e6N69O95//32T6+peffVVNG/eHHPnzsWgQYOQnZ2NunXrolOnToiLi0OrVq1Mltu0aVOzOhMTE9GxY0eL23Dw4EGTBxtHRUUBACIiIrB06dKq7poKUQghxH1dQzmKiopgZ2eHn376yeRpzBEREcjKysLGjRvN5nnyySfh6uoKOzs7bNy4Ee7u7hg6dCjefPNNs3RdnuzsbDg7O0Ov10vn4+khVdNPZKfKq+RT7IkeRgUFBTh//jwaNmxo8m0N9GC50+dYmexitRG7jIwMGAwGswsNPT09cfLkSYvznDt3Dj///DOGDRuGrVu34syZMxgzZgyKi4tNnlJ9q8LCQhQWFkrvs7Ozq28jiIiIiGoRq988URlGoxEeHh5YtGgRgoODMXjwYLz99ttYuHBhufNER0fD2dlZevn6+tZgxUREREQ1x2rBzs3NDSqVCqmpqSbtqamp5X6prre3Nx555BGT067NmzdHSkoKioqKLM4zdepU6PV66XX58uXq2wgiIiKiWsRqwU6j0SA4OBjx8fFSm9FoRHx8PDp16mRxnsceewxnzpyB0WiU2k6dOgVvb2/p60dup9Vq4eTkZPIiIiIikiOrnoqNiorC4sWLsWzZMpw4cQKvvPIK8vLypLtchg8fjqlTp0r9X3nlFWRmZmLChAk4deoUtmzZgg8++ABjx4611iYQERER1RpWfdzJ4MGDkZ6ejmnTpiElJQVBQUGIi4uTbqi4dOkSlMqb2dPX1xfbt2/Ha6+9htatW8PHxwcTJkzAm2++aa1NICIiqhVuPZtFD57q+vys9rgTa+HjTkjCx53UfnzcCdFdGY1GnD59GiqVCu7u7tBoNGbfhUq1lxACRUVFSE9Ph8FgkL5B41YPxONOiIiI6N4plUo0bNgQycnJuHr1qrXLoSqys7NDgwYNzEJdZTHYERERPeA0Gg0aNGiAkpISGAwGa5dDlaRSqWBjY1MtI60MdkRERDKgUCigVqvL/f5Sejg8UA8oJiIiIqLyMdgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyUStCHbz58+Hv78/dDodOnTogAMHDpTbd+nSpVAoFCYvnU5Xg9USERER1U5WD3arV69GVFQUpk+fjsOHD6NNmzbo2bMn0tLSyp3HyckJycnJ0uvixYs1WDERERFR7WT1YDd37lyMGjUKkZGRCAwMxMKFC2FnZ4fY2Nhy51EoFPDy8pJenp6eNVgxERERUe1k1WBXVFSEQ4cOITw8XGpTKpUIDw9HYmJiufPl5ubCz88Pvr6+6N+/P44fP15u38LCQmRnZ5u8iIiIiOTIqsEuIyMDBoPBbMTN09MTKSkpFudp2rQpYmNjsXHjRqxYsQJGoxGdO3fGlStXLPaPjo6Gs7Oz9PL19a327SAiIiKqDax+KrayOnXqhOHDhyMoKAihoaFYt24d3N3d8fXXX1vsP3XqVOj1eul1+fLlGq6YiIiIqGbYWHPlbm5uUKlUSE1NNWlPTU2Fl5dXhZahVqvRtm1bnDlzxuJ0rVYLrVZ7z7USERER1XZWHbHTaDQIDg5GfHy81GY0GhEfH49OnTpVaBkGgwF//fUXvL2971eZRERERA8Eq47YAUBUVBQiIiIQEhKC9u3bIyYmBnl5eYiMjAQADB8+HD4+PoiOjgYAzJo1Cx07dkRAQACysrIwZ84cXLx4ES+99JI1N4OIiIjI6qwe7AYPHoz09HRMmzYNKSkpCAoKQlxcnHRDxaVLl6BU3hxYvH79OkaNGoWUlBTUqVMHwcHB2LdvHwIDA621CURERES1gkIIIaxdRE3Kzs6Gs7Mz9Ho9nJycrF0OWdMMZ2tXQHczQ2/tCoiIrK4y2eWBuyuWiIiIiCxjsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIpmwsXYBREREdAcznK1dAd3NDL21K5BwxI6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSiVgS7+fPnw9/fHzqdDh06dMCBAwcqNN8PP/wAhUKBAQMG3N8CiYiIiB4AVg92q1evRlRUFKZPn47Dhw+jTZs26NmzJ9LS0u4434ULFzB58mQ8/vjjNVQpERERUe1m9WA3d+5cjBo1CpGRkQgMDMTChQthZ2eH2NjYcucxGAwYNmwYZs6ciUaNGtVgtURERES1l1WDXVFREQ4dOoTw8HCpTalUIjw8HImJieXON2vWLHh4eGDkyJE1USYRERHRA8HGmivPyMiAwWCAp6enSbunpydOnjxpcZ5ff/0VS5YswZEjRyq0jsLCQhQWFkrvs7Ozq1wvERERUW1m9VOxlZGTk4MXXngBixcvhpubW4XmiY6OhrOzs/Ty9fW9z1USERERWYdVR+zc3NygUqmQmppq0p6amgovLy+z/mfPnsWFCxfw1FNPSW1GoxEAYGNjg6SkJDRu3NhknqlTpyIqKkp6n52dzXBHREREsmTVYKfRaBAcHIz4+HjpkSVGoxHx8fEYN26cWf9mzZrhr7/+Mml75513kJOTg88++8xiYNNqtdBqtfelfiIiIqLaxKrBDgCioqIQERGBkJAQtG/fHjExMcjLy0NkZCQAYPjw4fDx8UF0dDR0Oh1atmxpMr+LiwsAmLUTERERPWysHuwGDx6M9PR0TJs2DSkpKQgKCkJcXJx0Q8WlS5egVD5QlwISERERWYVCCCGsXURNys7OhrOzM/R6PZycnKxdDlnTDGdrV0B3M0Nv7QqIrI/HqtrvPh+rKpNdOBRGREREJBMMdkREREQywWBHREREJBMMdkREREQywWBHREREJBMMdkREREQywWBHREREJBMMdkREREQywWBHREREJBPVEuyys7OxYcMGnDhxojoWR0RERERVUKVg9+yzz+LLL78EANy4cQMhISF49tln0bp1a6xdu7ZaCyQiIiKiiqlSsNuzZw8ef/xxAMD69eshhEBWVhY+//xzvPfee9VaIBERERFVTJWCnV6vh6urKwAgLi4OAwcOhJ2dHfr06YPTp09Xa4FEREREVDFVCna+vr5ITExEXl4e4uLi0KNHDwDA9evXodPpqrVAIiIiIqoYm6rMNHHiRAwbNgwODg5o0KABunbtCqD0FG2rVq2qsz4iIiIiqqAqBbsxY8agffv2uHz5Mrp37w6lsnTgr1GjRrzGjoiIiMhKqhTsACAkJAStW7fG+fPn0bhxY9jY2KBPnz7VWRsRERERVUKVrrHLz8/HyJEjYWdnhxYtWuDSpUsAgFdffRUffvhhtRZIRERERBVTpWA3depU/Pnnn0hISDC5WSI8PByrV6+utuKIiIiIqOKqdCp2w4YNWL16NTp27AiFQiG1t2jRAmfPnq224oiIiIio4qo0Ypeeng4PDw+z9ry8PJOgR0REREQ1p0rBLiQkBFu2bJHel4W5b775Bp06daqeyoiIiIioUqp0KvaDDz5A79698ffff6OkpASfffYZ/v77b+zbtw+//PJLdddIRERERBVQpRG7f/3rX/jzzz9RUlKCVq1aYceOHfDw8EBiYiKCg4Oru0YiIiIiqoBKj9gVFxdj9OjRePfdd7F48eL7URMRERERVUGlR+zUajXWrl17P2ohIiIiontQpVOxAwYMwIYNG6q5FCIiIiK6F1W6eaJJkyaYNWsWfvvtNwQHB8Pe3t5k+vjx46ulOCIiIiKquCoFuyVLlsDFxQWHDh3CoUOHTKYpFAoGu//nP2XL3TuR1VzQ3b0PERHRg6RKwe78+fPVXQcRERER3aMqXWN3KyEEhBDVUQsRERER3YMqB7vvvvsOrVq1gq2tLWxtbdG6dWssX768OmsjIiIiokqo0qnYuXPn4t1338W4cePw2GOPAQB+/fVXvPzyy8jIyMBrr71WrUUSERER0d1VKdh98cUXWLBgAYYPHy619evXDy1atMCMGTMY7IiIiIisoErBLjk5GZ07dzZr79y5M5KTk++5KCK5m3+gCHP2FSIlV6CNlxJf9LZFex9Vuf2zCgTeji/AupMlyLwh4OesREwvLZ5sojbr++GvhZgaX4gJHTSI6XXz1t+uS/Pwy0WDSd/RwWos7GsLAPgzxYAPfyvEr5cMyMgX8HdR4uVgNSZ01FbTVhMR0f1WpWAXEBCAH3/8EW+99ZZJ++rVq9GkSZNqKYxIrlYfK0bUjgIs7KNDh/oqxPyvCD1X5CFpnAM87M0vey0yCHRfngcPeyV+esYWPk5KXMwywkWnMOv7+z8GfH2oCK09LV8+O+pRNWaF3QxqduqbyziUbICHnRIr/q2Br7MS+y6X4D+bCqBSKjCuvaYatpyIiO63KgW7mTNnYvDgwdizZ490jd1vv/2G+Ph4/Pjjj9VaIJHczP1fIUY9qkZk29KwtLCvDltOlyD2j2JM+Zf56FjsH8XIvCGw70VbqFWlQczfxTy45RYJDFt3A4ufssV7ewotrttOrYCXg+XQ92Jb0/DWqI4GiZcNWHeimMGOiOgBUaW7YgcOHIj9+/fDzc0NGzZswIYNG+Dm5oYDBw7g3//+d3XXSCQbRQaBQ1eNCG90828qpUKB8EY2SLxisDjPf5NK0Km+DcZuLYDnJzlo+VUuPthbCIPR9DFDY7cWoE8TG5Nl3+77v4rh9nHpMqbuKkB+8Z0fVaQvBFxtzUcGiYiodqrSiB0ABAcHY8WKFdVZC5HsZeQLGATgaW8aljztFTiZYTnYnbtuxM/njRjWSo2tQ+1wJtOIMVsLUGwApnctHeH74VgxDicb8Psoe4vLAIChrdTwc1ainqMCR1ONeHNXAZKuGbFusJ3F/vsul2D18WJsGWp5OhER1T5VCnZbt26FSqVCz549Tdq3b98Oo9GI3r17V0txRAQYBeBhr8Cip3RQKRUIrqfCPzlGzNlXhOldtbisN2JCXAF2vmAHnU35o2v/Cb55OrWVpwrejgp0+y4fZzONaOxqOnh/LM2A/j/cwPRQLXo0rvLff0REVMOqdCp2ypQpMBjMRxeEEJgyZco9F0UkV252CqgUQGqe6SnQ1DxR7rVv3o4KPFJXCZXyZmhr7qZESq4oPbWbbEBansCjX+fBZlY2bGZl45eLBny+vwg2s7LNTtmW6fD/d+GeyTSatP+dbkC37/Lxn0fVeKcL74glInqQVOlP8dOnTyMwMNCsvVmzZjhz5sw9F0UkVxqVAsH1lIg/V4IBzUofVWIUAvHnSsq9QeExXxVW/lUMoxBQKkrD3alrRng7KKBRKdCtoQ3+esX0FGzkxhto5qbCm49pTALhrY6klP5x5u14c/rxNAOe+C4fEW3UeL+bzuJ8RERUe1Up2Dk7O+PcuXPw9/c3aT9z5gzs7cu/xoeIgKiOWkRsuIGQeiq09yl93ElesUBkUGnQG77+BnwcFYgOLw1Wr4Ro8OWBIkzYVoBXO2hw+poRH/xahPH/HwQdtQq09DB9Bp69WoG6tjfbz2YasfKvYjzZxAZ17RQ4mmrAa9sL0MVPhdaepX2OpRnwxLJ89AywQVQnDVJyS0fyVArA3cJjWIiIqPapUrDr378/Jk6ciPXr16Nx48YASkPdpEmT0K9fv2otkEhuBrdUIz1fYFpC6QOKg7yUiBtmB8//PxV7SW+EUnEzSPk6K7H9eTu8tr0QrRfkwcdJgQkdNHjzsYo/gkSjAnadL0HM/iLkFQn4OisxsLnpqdaf/i5Ger7AiqPFWHG0WGr3c1bgwkTHathyIiK63xRCiDs/78ACvV6PXr164eDBg6hfvz4A4PLly+jSpQvWrVsHFxeX6q6z2mRnZ8PZ2Rl6vR5OTk73dV3+U7bc1+XTvbmgG2rtEuhuZuitXQGR9c1wtnYFdDf3+VhVmexS5VOx+/btw86dO/Hnn3/C1tYWbdq0weOPP16lgomIiIjo3lXqwpnExERs3rwZAKBQKNCjRw94eHjgk08+wcCBA/Gf//wHhYWWn3hPRERERPdXpYLdrFmzcPz4cen9X3/9hVGjRqF79+6YMmUKNm3ahOjo6GovkoiIiIjurlLB7siRI+jWrZv0/ocffkD79u2xePFiREVF4fPPP+d3xRIRERFZSaWC3fXr1+Hp6Sm9/+WXX0y+ZaJdu3a4fPlypYuYP38+/P39odPp0KFDBxw4cKDcvuvWrUNISAhcXFxgb2+PoKAgLF++vNLrJCIiIpKbSgU7T09PnD9/HgBQVFSEw4cPo2PHjtL0nJwcqNXqShWwevVqREVFYfr06Th8+DDatGmDnj17Ii0tzWJ/V1dXvP3220hMTMTRo0cRGRmJyMhIbN++vVLrJSIiIpKbSgW7J598ElOmTMHevXsxdepU2NnZmdwJe/ToUem5dhU1d+5cjBo1CpGRkQgMDMTChQthZ2eH2NhYi/27du2Kf//732jevDkaN26MCRMmoHXr1vj1118rtV4iIiIiualUsJs9ezZsbGwQGhqKxYsXY/HixdBobj4kNTY2Fj169Kjw8oqKinDo0CGEh4ffLEipRHh4OBITE+86vxAC8fHxSEpKQpcuXSz2KSwsRHZ2tsmLiIiISI4q9Rw7Nzc37NmzB3q9Hg4ODlCpTL/GaM2aNXBwcKjw8jIyMmAwGEyu2wNKT/mePHmy3Pn0ej18fHxQWFgIlUqFr776Ct27d7fYNzo6GjNnzqxwTUREREQPqip9AaSzs7NZqANKr3+7dQTvfnF0dMSRI0fw+++/4/3330dUVBQSEhIs9p06dSr0er30qsrNHUREREQPgip980R1cXNzg0qlQmpqqkl7amoqvLy8yp1PqVQiICAAABAUFIQTJ04gOjoaXbt2Neur1Wqh1WrN2omIiIjkpkojdtVFo9EgODgY8fHxUpvRaER8fDw6depU4eUYjUZ+4wURERE99Kw6YgcAUVFRiIiIQEhICNq3b4+YmBjk5eUhMjISADB8+HD4+PhI32gRHR2NkJAQNG7cGIWFhdi6dSuWL1+OBQsWWHMziIiIiKzO6sFu8ODBSE9Px7Rp05CSkoKgoCDExcVJN1RcunQJSuXNgcW8vDyMGTMGV65cga2tLZo1a4YVK1Zg8ODB1toEIiIiolpBIYQQ1i6iJmVnZ8PZ2Rl6vR5OTk73dV3+U7bc1+XTvbmgG2rtEuhuZuitXQGR9c1wtnYFdDf3+VhVmexi1WvsiIiIiKj6MNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyYSNtQsgIiKi2mn+gSLM2VeIlFyBNl5KfNHbFu19VHed74djxRiy9gb6N7XBhufspPYRG25g2Z/FJn17NlYh7nl7AEDChRKELcu3uMwDL9mjnY8KSRkGvLylAH+nG6EvEKjnqMDQVmpMD9VCrVLcw9bKA4MdERERmVl9rBhROwqwsI8OHeqrEPO/IvRckYekcQ7wsC//hN+FLCMm7yjA4w0sB8BeASp8299Weq+9JYx19lUheZKDSf93fy5E/PkShNQrXadapcDw1mo86q2Ci06BP1MNGLWpAEYBfNBNdy+bLAsMdkRERGRm7v8KMepRNSLbagAAC/vqsOV0CWL/KMaUf2ktzmMwCgxbdwMzu2qx95IBWQXCrI9WpYCXg+VgqFEp4OVwM+gVGwQ2JpXg1fYaKBSl7Y3qKNGojkbq4+eiRMIFA/ZeMlR5W+WE19gRERGRiSKDwKGrRoQ3ujn+o1QoEN7IBolXyg9Qs34phIe9AiMf1ZTbJ+FCCTzm5KDpl7l4ZfMNXMs3ltv3v0kluHZDILKtutw+ZzKNiDtTglC/u58ifhhwxI6IiIhMZOQLGATgaW96zZqnvQInMywHu18vlWDJH8U48rJ9ucvtFWCDp5vboKGLEmevG/FWfCF6f5+PxJH2UCnNr49b8kcxeja2QX0n83GozkvycDjZgEID8J9H1ZgVZnkU8WHDYEdERET3JKdQ4IX1N7D4KR3c7Mo/Gfhcy5sjb608VWjtqULjz3ORcMGAbo1MI8mVbCO2ny3Bj4Nsb18MAGD1IFvkFAn8mWLE6zsL8Mm+IrzxGMMdgx0RERGZcLNTQKUAUvNMr5FLzRMWr487e92IC1kCT626AeAGAMD4/7PazMpG0jgHNHY1n69RHSXc7BQ4k2lEt0am0779oxh1bRXo19RyVPF1Ll1eoLsKBiHwn00FmNRJY3Hk72HCYEdEREQmNCoFguspEX+uBAOalY6yGYVA/LkSjGtvfv1cMzcl/nrF9BTsOz8XIqdI4LNeOvg6Ww5bV7KNuJYv4O1oOl0IgW+PFGF4G3WFHmFiFECxsfS/D/uVdgx2REREZCaqoxYRG24gpJ4K7X1KH3eSVywQGVQa9IavvwEfRwWiw3XQ2SjQ0sM0UrnoSgNZWXtukcDMhEIMDLSBl4MSZzONeGNXAQJclejZ2DSO/HzegPNZAi89an7TxPdHi6FWAa08lNDaKHDwqgFT4wsxuEXFQqDcMdgRERGRmcEt1UjPF5iWUPqA4iAvJeKG2cHz/0/FXtIboVRU/OEaKgVwNM2AZX8WI+v/Hyzco7ENZodpobUxDWRL/ihCZ18VmrmZj7/ZKIGPfivEqWtGCFH6uJNx7TR4rVP5d+I+TBRCCPOHzMhYdnY2nJ2dodfr4eTkdF/X5T9ly31dPt2bC7qh1i6B7maG3toVEFnfDGdrV0B3c5+PVZXJLnyOHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyUStCHbz58+Hv78/dDodOnTogAMHDpTbd/HixXj88cdRp04d1KlTB+Hh4XfsT0RERPSwsHqwW716NaKiojB9+nQcPnwYbdq0Qc+ePZGWlmaxf0JCAoYMGYLdu3cjMTERvr6+6NGjB/75558arpyIiIiodrF6sJs7dy5GjRqFyMhIBAYGYuHChbCzs0NsbKzF/t9//z3GjBmDoKAgNGvWDN988w2MRiPi4+NruHIiIiKi2sWqwa6oqAiHDh1CeHi41KZUKhEeHo7ExMQKLSM/Px/FxcVwdXW1OL2wsBDZ2dkmLyIiIiI5smqwy8jIgMFggKenp0m7p6cnUlJSKrSMN998E/Xq1TMJh7eKjo6Gs7Oz9PL19b3nuomIiIhqI6ufir0XH374IX744QesX78eOp3OYp+pU6dCr9dLr8uXL9dwlUREREQ1w8aaK3dzc4NKpUJqaqpJe2pqKry8vO447yeffIIPP/wQu3btQuvWrcvtp9VqodVqq6VeIiIiotrMqiN2Go0GwcHBJjc+lN0I0alTp3Ln+/jjjzF79mzExcUhJCSkJkolIiIiqvWsOmIHAFFRUYiIiEBISAjat2+PmJgY5OXlITIyEgAwfPhw+Pj4IDo6GgDw0UcfYdq0aVi5ciX8/f2la/EcHBzg4OBgte0gIiIisjarB7vBgwcjPT0d06ZNQ0pKCoKCghAXFyfdUHHp0iUolTcHFhcsWICioiIMGjTIZDnTp0/HjBkzarJ0IiIiolrF6sEOAMaNG4dx48ZZnJaQkGDy/sKFC/e/ICIiIqIH0AN9VywRERER3cRgR0RERCQTDHZEREREMsFgR0RERCQTteLmCap+OYc3Q79/HQx516HxaAjX8NHQ1mtqsW9+0j7o//cjiq8nA8YS2NSpB6d2/4ZDyycs9r+2/UvkHolDnSdGwaldfwBAiT4VWb/9gIJLR2HMuw6VgyvsA8Pg3PlZKFRqaV4hBLIPrEfun3EoyU6DytYJjm37wLnz4OrfCURERA8ZBjsZyjuxB5k/f4O6PcZCU68pcg5uRNqP01Bv1NdQ2buY9VfaOsC507NQu/oCKhvcOHsA17bGQGXnDNtGwSZ980/tQ+HVJKgcXE3ai69dAYRA3Z5jYVOnHorTL+Ja3BcQxQWo88RIqd/1+EW4cf4P1AkbCbW7H4wFuTDeyLkv+4GIiOhhw2AnQ9m/b4Bjm55waN0dAODacyxunP0duX/thHPHZ8z66xqYfiWbOqQ/8o79jMIrf5sEu5KcDGTu/Boez85C2k8zTeaxbRRs0lft4oXizCvI/WOrFOyKMy4j54+tqPfifKjr1q+27SUiIqJSvMZOZoShGEUpZ6DzC5LaFAoldP5BKPzn5N3nFwI3LhxBceYVaH1b3tJuRMbmuXDq8DQ07n4VqsVYmA+lraP0Pv/sfti4eOHG2QO4snAkrix4Ede2fQ4DR+yIiIiqBUfsZMaQnw0Io9kpV5WdS+np0nIYC/NwZX4EhKEYUChRt8crsG3YVpqe/b+foFCq4Bjcr0J1FF+/ipxDm1An7EWprSQrBSX6NOSd/A1ufV4DhBGZ8d8gfUM0vIZ8ULkNJSIiIjMMdgQAUGhs4R35OURRAQouHkHmz0tg4+IFXYPWKEw5g+xD/4V3xGdQKBR3XVZJTgbSfpwO+2b/gmNQr5sThAAMxXDrGwW1qw8AoG7v8UhZNhHF167w9CwREdE9YrCTGZWdE6BQwpCXZdJuyM+Cyr5OufMpFEqo69QDAGg8G6H42hXoE9eUBrvLx2HM0+OfBZE3ZxBGXN+9BNkHN6L+K7FSc0nONaSuegtan2Zw7WX6NXEqe1dAqZJCHQCo6/qWzpedzmBHRER0jxjsZEahUkPjFYCCi3/C7pFOAEqvjyu48Cccg/tWeDlCGEtPywKwbxkGnX8bk+lpP06DfYsn4NAqXGoryclA6qq3oPEKQN0nJ0KhML2EU1u/OWA0oPh6MtR1vEvnuf4PAMDG2aPyG0tEREQmGOxkyKndAGRsmQeNVxNovR9B9sGNEMUFUgjL2PwpVI51USd0BABAn/gjNF5NYFPHGygpxo1zvyPv+G649hgDAFDZOkFl62S6EqUNVPZ1pFG20lA3FTZOHqgT9iKM+dlSV5VD6Uihzj8IGs/GuLbtM7h2GwUhBDJ3LIDOv63JKB4RERFVDYOdDNk37wJDvh5Zv674/wcUN4LHs7OkU7El2enALaNpxuJCZO78Coaca1DYaKB2rQ+3vpNg37xLhddZcOEISq4no+R6Mv75aoTJNL83NwMoPd3rPnAaru/6Gikrp0Ch1sK2YYjJc+6IiIio6hRCCGHtImpSdnY2nJ2dodfr4eTkdPcZ7oH/lC33dfl0by7ohlq7BLqbGXprV0BkfTOcrV0B3c19PlZVJrvwOXZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTDHZEREREMmFj7QKIiMh6/KdssXYJdBcXdNaugB4kHLEjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgmrB7v58+fD398fOp0OHTp0wIEDB8rte/z4cQwcOBD+/v5QKBSIiYmpuUKJiIiIajmrBrvVq1cjKioK06dPx+HDh9GmTRv07NkTaWlpFvvn5+ejUaNG+PDDD+Hl5VXD1RIRERHVblYNdnPnzsWoUaMQGRmJwMBALFy4EHZ2doiNjbXYv127dpgzZw6ee+45aLXaGq6WiIiIqHazWrArKirCoUOHEB4efrMYpRLh4eFITEy0VllEREREDywba604IyMDBoMBnp6eJu2enp44efJkta2nsLAQhYWF0vvs7OxqWzYRERFRbWL1myfut+joaDg7O0svX19fa5dEREREdF9YLdi5ublBpVIhNTXVpD01NbVab4yYOnUq9Hq99Lp8+XK1LZuIiIioNrFasNNoNAgODkZ8fLzUZjQaER8fj06dOlXberRaLZycnExeRERERHJktWvsACAqKgoREREICQlB+/btERMTg7y8PERGRgIAhg8fDh8fH0RHRwMoveHi77//lv7/n3/+wZEjR+Dg4ICAgACrbQcRERFRbWDVYDd48GCkp6dj2rRpSElJQVBQEOLi4qQbKi5dugSl8uag4tWrV9G2bVvp/SeffIJPPvkEoaGhSEhIqOnyiYiIiGoVqwY7ABg3bhzGjRtncdrtYc3f3x9CiBqoioiIiOjBI/u7YomIiIgeFgx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDJh9a8UIyKih1PO4c3Q718HQ951aDwawjV8NLT1mlrsW5R+Efpfv0dhyhkYstNQ54lRcGrX36SPPvFH5J9KRHHmFShsNND6NEed0BFQ161vtjwhBNLWzEDB+UNw//fbsHukU+l60s5B/7+fUHjlbxhvZEPl5AHHtr3hFNLfbBlEtRFH7IiIqMblndiDzJ+/gctjQ+A94jNoPBoi7cdpMORlWewvSgph4+KFOqERUNnXsdin4PIxOD7aB17PfwLPwbMBQwlSf3wXxqICs745BzcCCvNlFKWcgcrOGW59J8F75Fdw7jwYWb98h+xDm+5lc4lqDIMdERHVuOzfN8CxTU84tO4OjVsDuPYcC4Vai9y/dlrsr/V+BHXCXoR9YCigUlvs4/nsLDi0CofG3Q8aj0ao2+c1GLLTUZR6xqRfUeo5ZB9YD7feE82W4dC6B1zDR0PXoBXULl5waBEG+1bhyD+VeM/bTFQTGOyIiKhGCUMxilLOQOcXJLUpFEro/INQ+M/JaluPsTAPAKDUOdxsKy5AxqY5cO3xClQOlkf+zOotzIPqlmUQ1WYMdkREVKMM+dmAMEJl72LSrrJzgSHverWsQwgjrscvhtYnEBp3f6n9evw30Po0h12TjhVaTsGVE8g7uRcOQb2qpS6i+43BjoiIZCdzxwIUpV+EW783pLb80/tRcOlP1Ok2qkLLKEq/gPR1s+H82BDYNnz0fpVKVK14VywREdUolZ0ToFCa3ShhyM8q98aIysjcuQA3zv4Oz6EfwsbJTWovuPgnSq6n4HLMYJP+6Ruioa0fCK+hH0ptRRmXkPrDO3AI6gWXzs/dc01ENYXBjoiIapRCpYbGKwAFF/+UHjMihBEFF/6EY3DfKi9XCIHruxYi/1QiPIdEQ+3iZTLdueMzcGjTw6QtOXYc6jzxEmwD2kttRekXkfrD23Bo+QTqdBle5XqIrIHBjoiIapxTuwHI2DIPGq8m0Ho/guyDGyGKC+DQKhwAkLH5U6gc66JO6AgApTdcFGdcLp3ZWAJD7jUUpZ6DQqODuk49AKUjdXl//wKPp9+BUmMHQ27p9XoKrR2Uai1UDnUs3jBh4+QuhcCi9AtI/eFt2DZ8FE7t/i0tA0olVHbO93GPEFUPBjsiIqpx9s27wJCvR9avK/7/AcWN4PHsLOlUbEl2OqC4eRm4ITcTyUvHS++zD6xD9oF10Pq2lE6h5v6xFQCQumqqybrqPjlRCox3k5/0G4z5euQd342847uldpWTB+q/Elu1jSWqQQohhLB2ETUpOzsbzs7O0Ov1cHJyuq/r8p+y5b4un+7NBd1Qa5dAdzNDb+0KZI/HqdqPx6oHwH0+VlUmu/CuWCIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikolaEezmz58Pf39/6HQ6dOjQAQcOHLhj/zVr1qBZs2bQ6XRo1aoVtm7dWkOVEhEREdVeVg92q1evRlRUFKZPn47Dhw+jTZs26NmzJ9LS0iz237dvH4YMGYKRI0fijz/+wIABAzBgwAAcO3ashisnIiIiql2sHuzmzp2LUaNGITIyEoGBgVi4cCHs7OwQGxtrsf9nn32GXr164fXXX0fz5s0xe/ZsPProo/jyyy9ruHIiIiKi2sXGmisvKirCoUOHMHXqVKlNqVQiPDwciYmJFudJTExEVFSUSVvPnj2xYcMGi/0LCwtRWFgovdfr9QCA7Ozse6z+7oyF+fd9HVR12Qph7RLobmrg9/Rhx+NU7cdj1QPgPh+ryjKLEHf/WbBqsMvIyIDBYICnp6dJu6enJ06ePGlxnpSUFIv9U1JSLPaPjo7GzJkzzdp9fX2rWDXJhbO1C6C7+5CfEhF/Cx4ANXSsysnJgbPznddl1WBXE6ZOnWoywmc0GpGZmYm6detCoVBYsTKypuzsbPj6+uLy5ctwcnKydjlERBbxWEVA6UhdTk4O6tWrd9e+Vg12bm5uUKlUSE1NNWlPTU2Fl5eXxXm8vLwq1V+r1UKr1Zq0ubi4VL1okhUnJyceLImo1uOxiu42UlfGqjdPaDQaBAcHIz4+XmozGo2Ij49Hp06dLM7TqVMnk/4AsHPnznL7ExERET0srH4qNioqChEREQgJCUH79u0RExODvLw8REZGAgCGDx8OHx8fREdHAwAmTJiA0NBQfPrpp+jTpw9++OEHHDx4EIsWLbLmZhARERFZndWD3eDBg5Geno5p06YhJSUFQUFBiIuLk26QuHTpEpTKmwOLnTt3xsqVK/HOO+/grbfeQpMmTbBhwwa0bNnSWptADyCtVovp06ebnaYnIqpNeKyiylKIitw7S0RERES1ntUfUExERERE1YPBjoiIiEgmGOyIiIiIZILBju5JQkICFAoFsrKy7tjP398fMTExNVKT3CgUinK/Mo+Iqh+Pa6W6du2KiRMnWrsMqiQGOwIALFy4EI6OjigpKZHacnNzoVar0bVrV5O+ZQe9s2fPonPnzkhOTpYenLh06VKrPgC6sgfa6OhoqFQqzJkzx2yapYNaRQ/4VTFjxgwEBQWZtScnJ6N3797Vvj4iuavNx7XExESoVCr06dPHbFp5x4L79Udeece1devWYfbs2dW+Prq/GOwIABAWFobc3FwcPHhQatu7dy+8vLywf/9+FBQUSO27d+9GgwYN0LhxY2g0Gnh5eT2wX88WGxuLN954A7GxsdYupVxeXl581AFRFdTm49qSJUvw6quvYs+ePbh69ep9W8+9cHV1haOjo7XLoMoSRP/P29tbREdHS+/feOMNMXbsWNG8eXOxe/duqb1Lly4iIiJCCCHE7t27BQBx/fp16f9vfU2fPl0IIYSfn594//33RWRkpHBwcBC+vr7i66+/Nln/0aNHRVhYmNDpdMLV1VWMGjVK5OTkSNNDQ0PFhAkTTObp37+/VEtoaKjZ+u8kISFB+Pj4iKKiIlGvXj3x22+/SdMiIiLMlnX+/HmztrJ1GwwG8cEHHwh/f3+h0+lE69atxZo1a6Tlle2bXbt2ieDgYGFrays6deokTp48KYQQ4ttvvzVb9rfffiuEEAKAWL9+fYX3U0REhOjfv7+YM2eO8PLyEq6urmLMmDGiqKjojvuDSI6sfVyzJCcnRzg4OIiTJ0+KwYMHi/fff1+aVt6xwM/Pz6TNz89PmmfDhg2ibdu2QqvVioYNG4oZM2aI4uJiaToAsXjxYjFgwABha2srAgICxMaNG4UQ4o7HtduPuZmZmeKFF14QLi4uwtbWVvTq1UucOnXKpHZnZ2cRFxcnmjVrJuzt7UXPnj3F1atX77pPqPow2JFk6NChokePHtL7du3aiTVr1oiXX35ZTJs2TQghRH5+vtBqtWLp0qVCCNMDYGFhoYiJiRFOTk4iOTlZJCcnS4HDz89PuLq6ivnz54vTp0+L6OhooVQqpWCTm5srvL29xdNPPy3++usvER8fLxo2bCgdYIS4e7C7du2aqF+/vpg1a5a0/jt54YUXxOTJk4UQQkyaNEm8+OKL0rSsrCzRqVMnMWrUKGlZJSUlYu3atQKASEpKEsnJySIrK0sIIcR7770nmjVrJuLi4sTZs2fFt99+K7RarUhISDDZTx06dBAJCQni+PHj4vHHHxedO3eW9uukSZNEixYtpPXl5+cLIUyDXUX2U0REhHBychIvv/yyOHHihNi0aZOws7MTixYtuuP+IJIjax7XyrNkyRIREhIihBBi06ZNonHjxsJoNEq1WDoWpKWlSSEvOTlZpKWlCSGE2LNnj3BychJLly4VZ8+eFTt27BD+/v5ixowZ0voAiPr164uVK1eK06dPi/HjxwsHBwdx7dq1Ox7Xbj/m9uvXTzRv3lzs2bNHHDlyRPTs2VMEBARIfzR+++23Qq1Wi/DwcPH777+LQ4cOiebNm4uhQ4dW+fOjymOwI8nixYuFvb29KC4uFtnZ2cLGxkakpaWJlStXii5dugghhIiPjxcAxMWLF4UQpgdAIW7+xXY7Pz8/8fzzz0vvjUaj8PDwEAsWLBBCCLFo0SJRp04dkZubK/XZsmWLUCqVIiUlRQhx92BXtp558+bddVv1er2wtbUVR44cEUII8ccffwgHB4e7jhDevr1CCFFQUCDs7OzEvn37TPqOHDlSDBkyxGS+Xbt2mWwfAHHjxg0hhBDTp08Xbdq0Mav11mBXkf0UEREh/Pz8RElJidTnmWeeEYMHD77rfiGSG2se18rTuXNnERMTI4QQori4WLi5uZmMHlbkWFCmW7du4oMPPjBpW758ufD29jaZ75133pHe5+bmCgBi27ZtFre3zK3HwFOnTgkAJmc2MjIyhK2trfjxxx+FEDdHG8+cOSP1mT9/vvD09Lzj/qDqxWvsSNK1a1fk5eXh999/x969e/HII4/A3d0doaGh0vUoCQkJaNSoERo0aFDp5bdu3Vr6f4VCAS8vL6SlpQEATpw4gTZt2sDe3l7q89hjj8FoNCIpKeneN+42q1atQuPGjdGmTRsAQFBQEPz8/LB69epKL+vMmTPIz89H9+7d4eDgIL2+++47nD171qTvrfvA29sbAKR9UBEV3U8tWrSASqUyWVdl1kMkF9Y8rlmSlJSEAwcOYMiQIQAAGxsbDB48GEuWLKn8xgH4888/MWvWLJNjz6hRo5CcnIz8/HyLddrb28PJyanSxx4bGxt06NBBaqtbty6aNm2KEydOSG12dnZo3Lix9J7Hnppn9e+KpdojICAA9evXx+7du3H9+nWEhoYCAOrVqwdfX1/s27cPu3fvxhNPPFGl5avVapP3CoUCRqOxwvMrlUqI274Br7i4uEq1LFmyBMePH4eNzc1fAaPRiNjYWIwcObJSy8rNzQUAbNmyBT4+PibTbr/p4dZ9UHZhdmX2QUXd674mkovadlxbsmQJSkpKUK9ePalNCAGtVosvv/xSuhO3onJzczFz5kw8/fTTZtN0Ol2V66wqS+u5/bhN9xeDHZkICwtDQkICrl+/jtdff11q79KlC7Zt24YDBw7glVdeKXd+jUYDg8FQ6fU2b94cS5cuRV5enjQa9dtvv0GpVKJp06YAAHd3dyQnJ0vzGAwGHDt2DGFhYZVa/19//YWDBw8iISEBrq6uUntmZia6du2KkydPolmzZhaXpdFopHWXCQwMhFarxaVLl6R/NKqiIrVXZD8RkSlrHdduV1JSgu+++w6ffvopevToYTJtwIABWLVqFV5++eVy16dWq83aH330USQlJSEgIKDKdVk6rt2uefPmKCkpwf79+9G5c2cAwLVr15CUlITAwMAqr5uqH0/FkomwsDD8+uuvOHLkiElICQ0Nxddff42ioiKTIHU7f39/5ObmIj4+HhkZGSanAu5k2LBh0Ol0iIiIwLFjx7B79268+uqreOGFF+Dp6QkAeOKJJ7BlyxZs2bIFJ0+exCuvvGL23CV/f3/s2bMH//zzDzIyMiyua8mSJWjfvj26dOmCli1bSq8uXbqgXbt20ikRf39/7N+/HxcuXEBGRgaMRiP8/PygUCiwefNmpKenIzc3F46Ojpg8eTJee+01LFu2DGfPnsXhw4fxxRdfYNmyZRXa/rL1nT9/HkeOHEFGRgYKCwurtJ+IyJS1jmu327x5M65fv46RI0eaHHtatmyJgQMHmhx7LB0L/P39ER8fj5SUFFy/fh0AMG3aNHz33XeYOXMmjh8/jhMnTuCHH37AO++8U+G6LB3XbtekSRP0798fo0aNwq+//oo///wTzz//PHx8fNC/f/8q7Q+6PxjsyERYWBhu3LiBgIAAk6AQGhqKnJwcNG3aVLo2zJLOnTvj5ZdfxuDBg+Hu7o6PP/64Quu1s7PD9u3bkZmZiXbt2mHQoEHo1q0bvvzyS6nPiy++iIiICAwfPhyhoaFo1KiR2cF41qxZuHDhAho3bgx3d3ez9RQVFWHFihUYOHCgxToGDhyI7777DsXFxZg8eTJUKhUCAwPh7u6OS5cuwcfHBzNnzsSUKVPg6emJcePGAQBmz56Nd999F9HR0WjevDl69eqFLVu2oGHDhhXa/rJ19+rVC2FhYXB3d8eqVauqtJ+IyJS1jmu3W7JkCcLDwy2ebh04cCAOHjyIo0ePlnss+PTTT7Fz5074+vqibdu2AICePXti8+bN2LFjB9q1a4eOHTti3rx58PPzq3Bd5R3Xbvftt98iODgYffv2RadOnSCEwNatW81Ov5J1KQRPfhMRERHJAkfsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJv4P/JXtlGlE090AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training times in seconds\n",
        "without_attention_time = 1196.70\n",
        "\n",
        "# From the log, these are the per-epoch times for \"With Attention\"\n",
        "with_attention_times = [63, 81, 82, 60, 83, 81, 60, 83, 81, 60, 83, 81, 81, 81, 82]\n",
        "with_attention_total = sum(with_attention_times)\n",
        "\n",
        "# Plotting\n",
        "models = ['Without Attention', 'With Attention']\n",
        "times = [without_attention_time, with_attention_total]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(models, times, color=['skyblue', 'lightcoral'])\n",
        "plt.ylabel('Training Time (seconds)')\n",
        "plt.title('Training Time Comparison: With vs Without Attention')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 10, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xxbBsBFD5H4J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}